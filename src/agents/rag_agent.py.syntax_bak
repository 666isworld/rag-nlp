import os
from typing import List
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader

from src.prompts.rag_prompts import RAGPromptTemplates
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.memory import ConversationBufferMemory
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
# 导入新的文档加载器工具
from src.utils.document_loaders import get_document_loader

class RAGAgent:
    def __init__(
        self, 
        docs_dir: str = "docs", 
        persist_dir: str = "db", 
        api_base: str = None, 
        api_key: str = None
    ):
        load_dotenv()
        
        self.docs_dir = docs_dir
        self.persist_dir = persist_dir
        
        # Create directories if they don't exist
        os.makedirs(docs_dir, exist_ok=True)
        os.makedirs(persist_dir, exist_ok=True)
        
        # Initialize components
        self.embeddings = HuggingFaceEmbeddings(model_name="d:/code/python/rag-knowledge-base/models/all-MiniLM-L6-v2")
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        
        # Initialize vector store
        self.vector_store = self._initialize_vector_store()
        
        # Initialize LLM with custom API if provided
        if api_base and api_key:
            self.llm = ChatOpenAI(
                temperature=0.7,
                openai_api_base=api_base,
                openai_api_key=api_key
            )
        else:
            self.llm = ChatOpenAI(temperature=0.7)
            
        # self.memory = None # 已禁用内存组件(
            memory_key="chat_history",
            return_messages=True
        )
        
        # Initialize retrieval chain
        self.qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vector_store.as_retriever(),
            # 已禁用内存组件
        )

    def _initialize_vector_store(self) -> Chroma:
        """Initialize or load the vector store."""
        # Check if vector store exists
        if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):
            return Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embeddings
            )
        
        # 使用增强的文档加载器
        documents = get_document_loader(self.docs_dir)
        
        if not documents:
            print(f"警告: 在{self.docs_dir}目录中未找到任何文档")
            return Chroma(
                persist_directory=self.persist_dir,
                embedding_function=self.embeddings
            )
        
        # Split documents
        splits = self.text_splitter.split_documents(documents)
        
        # Create and persist vector store
        vector_store = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory=self.persist_dir
        )
        vector_store.persist()
        return vector_store

    def query(self, question: str) -> str:
        """Query the RAG system with a question."""
        response = self.qa_chain({"question": question})
        return response["answer"]

    def ingest_documents(self) -> None:
        """Reinitialize the vector store with current documents."""
        if os.path.exists(self.persist_dir):
            import shutil
            shutil.rmtree(self.persist_dir)
        self.vector_store = self._initialize_vector_store()